{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a8f04b84ee854c8dabfc7afa63f7f027": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_06e1bc462bda41db957aa7775f3123ab",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Training... \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training... <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">  0%</span> <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "06e1bc462bda41db957aa7775f3123ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmzufljHWo4A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "from typing import Iterator, Tuple, Union, Dict, List\n",
        "from argparse import ArgumentParser, Namespace\n",
        "from collections import OrderedDict, Counter\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fedlab~=1.1.4\n",
        "from fedlab.utils.serialization import SerializationTool\n",
        "from fedlab.utils.aggregator import Aggregators\n",
        "from fedlab.utils.dataset.slicing import noniid_slicing"
      ],
      "metadata": {
        "id": "MHgGoPWjX4oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f5386d-0bf7-493b-bcdb-e202200e1550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fedlab~=1.1.4\n",
            "  Downloading fedlab-1.1.5.tar.gz (36 kB)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (0.14.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (1.3.5)\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.1->fedlab~=1.1.4) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.8.2->fedlab~=1.1.4) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.8.2->fedlab~=1.1.4) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fedlab~=1.1.4) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fedlab~=1.1.4) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->fedlab~=1.1.4) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->fedlab~=1.1.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->fedlab~=1.1.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->fedlab~=1.1.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->fedlab~=1.1.4) (2022.12.7)\n",
            "Building wheels for collected packages: fedlab\n",
            "  Building wheel for fedlab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fedlab: filename=fedlab-1.1.5-py3-none-any.whl size=56049 sha256=18fa2dfdb94615a20ec9e606522a561c60facfb48780a29c36b8ec4d20ca4df7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/fa/bf/095e6b35c4cfcfe0eed549aea1a2b280bc7617f86c6e165810\n",
            "Successfully built fedlab\n",
            "Installing collected packages: pynvml, fedlab\n",
            "Successfully installed fedlab-1.1.5 pynvml-11.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rich\n",
        "import rich\n",
        "from rich.console import Console\n",
        "from rich.progress import track"
      ],
      "metadata": {
        "id": "fe-Ub3lGYBit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb59b8d-a687-4de4-fa00-1089d7908587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rich\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 29.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich) (4.4.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: commonmark, rich\n",
            "Successfully installed commonmark-0.9.1 rich-12.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install path\n",
        "from path import Path"
      ],
      "metadata": {
        "id": "VnmJGjICYteL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e7a1761-28ff-4acb-96e7-5881ccea86c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting path\n",
            "  Downloading path-16.6.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: path\n",
            "Successfully installed path-16.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MOUNT"
      ],
      "metadata": {
        "id": "x_hAP6OFyzLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/Shareddrives/Duong-DatDeakin/Personalized_FedAvg'"
      ],
      "metadata": {
        "id": "PQpR9mnAy1HS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc24d02b-7c5e-4db2-f356-d231e014fadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "o_LO2k2iz6Xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d217d163-6b7d-4b55-cb7c-de611f6be498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Duong-DatDeakin/Personalized_FedAvg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA/DATASET"
      ],
      "metadata": {
        "id": "Ms1kWZKeY2Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        subset=None,\n",
        "        data=None,\n",
        "        targets=None,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "    ) -> None:\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        if (data is not None) and (targets is not None):\n",
        "            self.data = data.unsqueeze(1)\n",
        "            self.targets = targets\n",
        "        elif subset is not None:\n",
        "            self.data = torch.stack(\n",
        "                list(\n",
        "                    map(\n",
        "                        lambda tup: tup[0]\n",
        "                        if isinstance(tup[0], torch.Tensor)\n",
        "                        else torch.tensor(tup[0]),\n",
        "                        subset,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            self.targets = torch.stack(\n",
        "                list(\n",
        "                    map(\n",
        "                        lambda tup: tup[1]\n",
        "                        if isinstance(tup[1], torch.Tensor)\n",
        "                        else torch.tensor(tup[1]),\n",
        "                        subset,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Data Format: subset: Tuple(data: Tensor / Image / np.ndarray, targets: Tensor) OR data: List[Tensor]  targets: List[Tensor]\"\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data, targets = self.data[index], self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            data = self.transform(self.data[index])\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            targets = self.target_transform(self.targets[index])\n",
        "\n",
        "        return data, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "\n",
        "class CIFARDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        subset=None,\n",
        "        data=None,\n",
        "        targets=None,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "    ) -> None:\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        if (data is not None) and (targets is not None):\n",
        "            self.data = data.unsqueeze(1)\n",
        "            self.targets = targets\n",
        "        elif subset is not None:\n",
        "            self.data = torch.stack(\n",
        "                list(\n",
        "                    map(\n",
        "                        lambda tup: tup[0]\n",
        "                        if isinstance(tup[0], torch.Tensor)\n",
        "                        else torch.tensor(tup[0]),\n",
        "                        subset,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            self.targets = torch.stack(\n",
        "                list(\n",
        "                    map(\n",
        "                        lambda tup: tup[1]\n",
        "                        if isinstance(tup[1], torch.Tensor)\n",
        "                        else torch.tensor(tup[1]),\n",
        "                        subset,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Data Format: subset: Tuple(data: Tensor / Image / np.ndarray, targets: Tensor) OR data: List[Tensor]  targets: List[Tensor]\"\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, targets = self.data[index], self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(self.data[index])\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            targets = self.target_transform(self.targets[index])\n",
        "\n",
        "        return img, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n"
      ],
      "metadata": {
        "id": "OGJ04z7GY7nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PREPROCESS"
      ],
      "metadata": {
        "id": "taZ4Ge-SJVqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CURRENT_DIR = Path(__file__).parent.abspath()\n",
        "CURRENT_DIR = \"/content/drive/Shareddrives/Duong-DatDeakin/Personalized_FedAvg\"\n",
        "\n",
        "DATASET = {\n",
        "    \"mnist\": (MNIST, MNISTDataset),\n",
        "    \"cifar\": (CIFAR10, CIFARDataset),\n",
        "}\n",
        "\n",
        "MEAN = {\n",
        "    \"mnist\": (0.1307,),\n",
        "    \"cifar\": (0.4914, 0.4822, 0.4465),\n",
        "}\n",
        "\n",
        "STD = {\n",
        "    \"mnist\": (0.3015,),\n",
        "    \"cifar\": (0.2023, 0.1994, 0.2010),\n",
        "}\n",
        "\n",
        "def preprocess(args: Namespace) -> None:\n",
        "    dataset_dir = CURRENT_DIR + \"/\" + args.dataset\n",
        "    pickles_dir = CURRENT_DIR + \"/\" + args.dataset + \"/\" + \"pickles\"\n",
        "\n",
        "    np.random.seed(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    num_train_clients = int(args.client_num_in_total * args.fraction)\n",
        "    num_test_clients = args.client_num_in_total - num_train_clients\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Normalize(MEAN[args.dataset], STD[args.dataset]),]\n",
        "    )\n",
        "    target_transform = None\n",
        "    trainset_stats = {}\n",
        "    testset_stats = {}\n",
        "\n",
        "    if not os.path.isdir(CURRENT_DIR + \"/\" + args.dataset):\n",
        "        os.mkdir(CURRENT_DIR + \"/\" + args.dataset)\n",
        "    if os.path.isdir(pickles_dir):\n",
        "        os.system(f\"rm -rf {pickles_dir}\")\n",
        "    os.mkdir(f\"{pickles_dir}\")\n",
        "\n",
        "    ori_dataset, target_dataset = DATASET[args.dataset]\n",
        "    trainset = ori_dataset(\n",
        "        dataset_dir, train=True, download=True, transform=transforms.ToTensor()\n",
        "    )\n",
        "    testset = ori_dataset(dataset_dir, train=False, transform=transforms.ToTensor())\n",
        "\n",
        "    num_classes = 10 if args.classes <= 0 else args.classes\n",
        "    all_trainsets, trainset_stats = randomly_alloc_classes(\n",
        "        ori_dataset=trainset,\n",
        "        target_dataset=target_dataset,\n",
        "        num_clients=num_train_clients,\n",
        "        num_classes=num_classes,\n",
        "        transform=transform,\n",
        "        target_transform=target_transform,\n",
        "    )\n",
        "    all_testsets, testset_stats = randomly_alloc_classes(\n",
        "        ori_dataset=testset,\n",
        "        target_dataset=target_dataset,\n",
        "        num_clients=num_test_clients,\n",
        "        num_classes=num_classes,\n",
        "        transform=transform,\n",
        "        target_transform=target_transform,\n",
        "    )\n",
        "\n",
        "    all_datasets = all_trainsets + all_testsets\n",
        "\n",
        "    for client_id, dataset in enumerate(all_datasets):\n",
        "        with open(pickles_dir + \"/\" + str(client_id) + \".pkl\", \"wb\") as f:\n",
        "            pickle.dump(dataset, f)\n",
        "    with open(pickles_dir + \"/\" + \"seperation.pkl\", \"wb\") as f:\n",
        "        pickle.dump(\n",
        "            {\n",
        "                \"train\": [i for i in range(num_train_clients)],\n",
        "                \"test\": [i for i in range(num_train_clients, args.client_num_in_total)],\n",
        "                \"total\": args.client_num_in_total,\n",
        "            },\n",
        "            f,\n",
        "        )\n",
        "    with open(dataset_dir + \"/\" + \"all_stats.json\", \"w\") as f:\n",
        "        json.dump({\"train\": trainset_stats, \"test\": testset_stats}, f)\n",
        "\n",
        "def randomly_alloc_classes(\n",
        "    ori_dataset: Dataset,\n",
        "    target_dataset: Dataset,\n",
        "    num_clients: int,\n",
        "    num_classes: int,\n",
        "    transform=None,\n",
        "    target_transform=None,\n",
        ") -> Tuple[List[Dataset], Dict[str, Dict[str, int]]]:\n",
        "    dict_users = noniid_slicing(ori_dataset, num_clients, num_clients * num_classes)\n",
        "    stats = {}\n",
        "    for i, indices in dict_users.items():\n",
        "        targets_numpy = np.array(ori_dataset.targets)\n",
        "        stats[f\"client {i}\"] = {\"x\": 0, \"y\": {}}\n",
        "        stats[f\"client {i}\"][\"x\"] = len(indices)\n",
        "        stats[f\"client {i}\"][\"y\"] = Counter(targets_numpy[indices].tolist())\n",
        "    datasets = []\n",
        "    for indices in dict_users.values():\n",
        "        datasets.append(\n",
        "            target_dataset(\n",
        "                [ori_dataset[i] for i in indices],\n",
        "                transform=transform,\n",
        "                target_transform=target_transform,\n",
        "            )\n",
        "        )\n",
        "    return datasets, stats\n",
        "\n",
        "class get_args_preprocess():\n",
        "  def __init__(self):\n",
        "    self.dataset = \"mnist\"\n",
        "    self.client_num_in_total = 200\n",
        "    self.fraction = 0.9\n",
        "    self.classes = 2\n",
        "    self.seed = 0\n",
        "\n",
        "args = get_args_preprocess()\n",
        "preprocess(args)"
      ],
      "metadata": {
        "id": "2dU0ag3wJY-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bba639-8929-48af-a29f-d9a6bb704a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fedlab/utils/dataset/slicing.py:38: UserWarning: warning: the length of dataset isn't divided exactly by num_shard.some samples will be dropped.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA/UTILS"
      ],
      "metadata": {
        "id": "UJKhRhvnQHU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DICT = {\n",
        "    \"mnist\": MNISTDataset,\n",
        "    \"cifar\": CIFARDataset,\n",
        "}\n",
        "#CURRENT_DIR = Path(__file__).parent.abspath()\n",
        "CURRENT_DIR = \"/content/drive/Shareddrives/Duong-DatDeakin/Personalized_FedAvg\"\n",
        "\n",
        "def get_dataloader(dataset: str, client_id: int, batch_size=20, valset_ratio=0.1):\n",
        "    pickles_dir = CURRENT_DIR + \"/\" + dataset + \"/\" \"pickles\"\n",
        "    if os.path.isdir(pickles_dir) is False:\n",
        "        raise RuntimeError(\"Please preprocess and create pickles first.\")\n",
        "\n",
        "    with open(pickles_dir + \"/\" + str(client_id) + \".pkl\", \"rb\") as f:\n",
        "        client_dataset: DATASET_DICT[dataset] = pickle.load(f)\n",
        "\n",
        "    val_num_samples = int(valset_ratio * len(client_dataset))\n",
        "    train_num_samples = len(client_dataset) - val_num_samples\n",
        "\n",
        "    trainset, valset = random_split(\n",
        "        client_dataset, [train_num_samples, val_num_samples]\n",
        "    )\n",
        "    trainloader = DataLoader(trainset, batch_size, drop_last=True)\n",
        "    valloader = DataLoader(valset, batch_size)\n",
        "\n",
        "    return trainloader, valloader\n",
        "\n",
        "def get_client_id_indices(dataset):\n",
        "    dataset_pickles_path = CURRENT_DIR + \"/\" + dataset + \"/\" + \"pickles\"\n",
        "    with open(dataset_pickles_path + \"/\" + \"seperation.pkl\", \"rb\") as f:\n",
        "        seperation = pickle.load(f)\n",
        "    return (seperation[\"train\"], seperation[\"test\"], seperation[\"total\"])"
      ],
      "metadata": {
        "id": "_buZQAOvYP0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MODEL"
      ],
      "metadata": {
        "id": "8d1jlnQ9OuBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class elu(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(elu, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.where(x >= 0, x, 0.2 * (torch.exp(x) - 1))\n",
        "\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self, in_c, out_c) -> None:\n",
        "        super(linear, self).__init__()\n",
        "        self.w = nn.Parameter(\n",
        "            torch.randn(out_c, in_c) * torch.sqrt(torch.tensor(2 / in_c))\n",
        "        )\n",
        "        self.b = nn.Parameter(torch.randn(out_c))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.w, self.b)\n",
        "\n",
        "\n",
        "class MLP_MNIST(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(MLP_MNIST, self).__init__()\n",
        "        self.fc1 = linear(28 * 28, 80)\n",
        "        self.fc2 = linear(80, 60)\n",
        "        self.fc3 = linear(60, 10)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.activation = elu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP_CIFAR10(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(MLP_CIFAR10, self).__init__()\n",
        "        self.fc1 = linear(32 * 32 * 3, 80)\n",
        "        self.fc2 = linear(80, 60)\n",
        "        self.fc3 = linear(60, 10)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.activation = elu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "MODEL_DICT = {\"mnist\": MLP_MNIST, \"cifar\": MLP_CIFAR10}\n",
        "\n",
        "def get_model(dataset, device):\n",
        "    return MODEL_DICT[dataset]().to(device)"
      ],
      "metadata": {
        "id": "cQ7SZ-9gOwSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###UTILS"
      ],
      "metadata": {
        "id": "UE-4H9KGQm7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class get_args():\n",
        "  def __init__(self):\n",
        "    self.alpha = 1e-2\n",
        "    self.beta = 1e-3\n",
        "    self.global_epochs = 200\n",
        "    self.local_epochs = 4\n",
        "    self.pers_epochs = 1\n",
        "    self.hf = 1\n",
        "    self.batch_size = 40\n",
        "    self.valset_ratio = 0.1\n",
        "    self.dataset = \"mnist\"  #choices = [\"mnist\", \"cifar\"]\n",
        "    self.client_num_per_round = 10\n",
        "    self.seed = 17\n",
        "    self.gpu = 1\n",
        "    self.eval_while_training = 1\n",
        "    self.log = 0\n",
        "    self.mix = 1\n",
        "    \n",
        "# def get_args():\n",
        "#     parser = ArgumentParser()\n",
        "#     parser.add_argument(\"--alpha\", type=float, default=1e-2)\n",
        "#     parser.add_argument(\"--beta\", type=float, default=1e-3)\n",
        "#     parser.add_argument(\"--global_epochs\", type=int, default=200)\n",
        "#     parser.add_argument(\"--local_epochs\", type=int, default=4)\n",
        "#     parser.add_argument(\n",
        "#         \"--pers_epochs\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=\"Indicate how many data batches would be used for personalization. Negatives means that equal to train phase.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--hf\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=\"0 for performing Per-FedAvg(FO), others for Per-FedAvg(HF)\",\n",
        "#     )\n",
        "#     parser.add_argument(\"--batch_size\", type=int, default=40)\n",
        "#     parser.add_argument(\n",
        "#         \"--valset_ratio\",\n",
        "#         type=float,\n",
        "#         default=0.1,\n",
        "#         help=\"Proportion of val set in the entire client local dataset\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--dataset\", type=str, choices=[\"mnist\", \"cifar\"], default=\"mnist\"\n",
        "#     )\n",
        "#     parser.add_argument(\"--client_num_per_round\", type=int, default=10)\n",
        "#     parser.add_argument(\"--seed\", type=int, default=17)\n",
        "#     parser.add_argument(\n",
        "#         \"--gpu\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=\"Non-zero value for using gpu, 0 for using cpu\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--eval_while_training\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=\"Non-zero value for performing local evaluation before and after local training\",\n",
        "#     )\n",
        "#     parser.add_argument(\"--log\", type=int, default=0)\n",
        "#     return parser.parse_args()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: Union[torch.nn.MSELoss, torch.nn.CrossEntropyLoss],\n",
        "    device=torch.device(\"cpu\"),\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_samples = 0\n",
        "    acc = 0\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logit = model(x)\n",
        "        # total_loss += criterion(logit, y) / y.size(-1)\n",
        "        total_loss += criterion(logit, y)\n",
        "        pred = torch.softmax(logit, -1).argmax(-1)\n",
        "        acc += torch.eq(pred, y).int().sum()\n",
        "        num_samples += y.size(-1)\n",
        "    model.train()\n",
        "    return total_loss, acc / num_samples\n",
        "\n",
        "def get_data_batch(\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    iterator: Iterator,\n",
        "    device=torch.device(\"cpu\"),\n",
        "):\n",
        "    try:\n",
        "        x, y = next(iterator)\n",
        "    except StopIteration:\n",
        "        iterator = iter(dataloader)\n",
        "        x, y = next(iterator)\n",
        "\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "def fix_random_seed(seed: int):\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "xV8kxW7kQoJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Per-FedAvg"
      ],
      "metadata": {
        "id": "nAX4Nd9VPYYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Beta\n",
        "\n",
        "class ClientInterpolation:\n",
        "    def __init__(self):\n",
        "        self.dist = Beta(torch.FloatTensor([2]), torch.FloatTensor([2]))\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[2]\n",
        "        H = size[3]\n",
        "        cut_rat = np.sqrt(1. - lam.cpu())\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "\n",
        "        # uniform\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "    def mixup_data(self, x_support, x_query, lam):\n",
        "        mixed_x = x_query.clone()\n",
        "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(x_query.size(), lam)\n",
        "\n",
        "        mixed_x[:, :, bbx1:bbx2, bby1:bby2] = x_support[:, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x_query.size()[-1] * x_query.size()[-2]))\n",
        "\n",
        "        return mixed_x, lam        \n",
        "\n",
        "    # def client_crossmix(self, x1s, y1s, x1q, y1q, x2s, y2s, x2q, y2q):\n",
        "        \n",
        "    #     return None"
      ],
      "metadata": {
        "id": "z34ekkq_yIFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerFedAvgClient(ClientInterpolation):\n",
        "    def __init__(\n",
        "        self,\n",
        "        client_id: int,\n",
        "        alpha: float,\n",
        "        beta: float,\n",
        "        global_model: torch.nn.Module,\n",
        "        criterion: Union[torch.nn.CrossEntropyLoss, torch.nn.MSELoss],\n",
        "        batch_size: int,\n",
        "        dataset: str,\n",
        "        local_epochs: int,\n",
        "        valset_ratio: float,\n",
        "        # qs_ratio: float \n",
        "        logger: rich.console.Console,\n",
        "        gpu: int,\n",
        "    ):\n",
        "        if gpu and torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "        self.logger = logger\n",
        "        self.dataset = dataset\n",
        "        self.valset_ratio = valset_ratio\n",
        "        self.local_epochs = local_epochs\n",
        "        self.criterion = criterion\n",
        "        self.id = client_id\n",
        "        self.model = deepcopy(global_model)\n",
        "        self.alpha = alpha                  # inner loop learning rate\n",
        "        self.beta = beta                    # meta  loop learning rate\n",
        "        self.batch_size = batch_size\n",
        "        self.o_trainloader, self.valloader = get_dataloader(\n",
        "            dataset, client_id, batch_size, valset_ratio\n",
        "        )\n",
        "        self.dist = Beta(torch.FloatTensor([2]), torch.FloatTensor([2]))\n",
        "        # self.iter_trainloader = iter(self.trainloader)\n",
        "        self.num_classes = 10\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        global_model: torch.nn.Module,\n",
        "        hessian_free=False,\n",
        "        eval_while_training=False,\n",
        "    ):\n",
        "        self.model.load_state_dict(global_model.state_dict())\n",
        "        if eval_while_training:\n",
        "            loss_before, acc_before = eval(\n",
        "                self.model, self.valloader, self.criterion, self.device\n",
        "            )\n",
        "        self._train(hessian_free)\n",
        "\n",
        "        if eval_while_training:\n",
        "            loss_after, acc_after = eval(\n",
        "                self.model, self.valloader, self.criterion, self.device\n",
        "            )\n",
        "            self.logger.log(\n",
        "                \"client [{}] [red]loss: {:.4f} -> {:.4f}   [blue]acc: {:.2f}% -> {:.2f}%\".format(\n",
        "                    self.id,\n",
        "                    loss_before,\n",
        "                    loss_after,\n",
        "                    acc_before * 100.0,\n",
        "                    acc_after * 100.0,\n",
        "                )\n",
        "            )\n",
        "        return SerializationTool.serialize_model(self.model)\n",
        "\n",
        "    def _train(self, hessian_free=False):\n",
        "        if args.mix:\n",
        "            self.trainloader = self.o_trainloader\n",
        "            # self.iter_trainloader = iter(self.trainloader)\n",
        "        else: \n",
        "            self.trainloader = self.cr_trainloader\n",
        "        self.iter_trainloader = iter(self.trainloader)\n",
        "        if hessian_free:  # Per-FedAvg(HF)\n",
        "            for _ in range(self.local_epochs):\n",
        "                temp_model = deepcopy(self.model)\n",
        "                # 1st inner-loop\n",
        "                data_batch_1 = get_data_batch(\n",
        "                    self.trainloader, self.iter_trainloader, self.device\n",
        "                )\n",
        "                grads = self.compute_grad(temp_model, data_batch_1)\n",
        "                for param, grad in zip(temp_model.parameters(), grads):\n",
        "                    param.data.sub_(self.alpha * grad)\n",
        "                # 2nd \n",
        "                data_batch_2 = get_data_batch(\n",
        "                    self.trainloader, self.iter_trainloader, self.device\n",
        "                )\n",
        "                grads_1st = self.compute_grad(temp_model, data_batch_2)\n",
        "                # 3rd \n",
        "                data_batch_3 = get_data_batch(\n",
        "                    self.trainloader, self.iter_trainloader, self.device\n",
        "                )\n",
        "\n",
        "                grads_2nd = self.compute_grad(\n",
        "                    self.model, data_batch_3, v=grads_1st, second_order_grads=True\n",
        "                )\n",
        "                for param, grad1, grad2 in zip(\n",
        "                    self.model.parameters(), grads_1st, grads_2nd\n",
        "                ):\n",
        "                    param.data.sub_(self.beta * grad1 - self.beta * self.alpha * grad2)\n",
        "\n",
        "        else:  # Per-FedAvg(FO)\n",
        "            for _ in range(self.local_epochs):\n",
        "                # ========================== FedAvg ==========================\n",
        "                # NOTE: You can uncomment those codes for running FedAvg.\n",
        "                #       When you're trying to run FedAvg, comment other codes in this branch.\n",
        "\n",
        "                # data_batch = utils.get_data_batch(\n",
        "                #     self.trainloader, self.iter_trainloader, self.device\n",
        "                # )\n",
        "                # grads = self.compute_grad(self.model, data_batch)\n",
        "                # for param, grad in zip(self.model.parameters(), grads):\n",
        "                #     param.data.sub_(self.beta * grad)\n",
        "\n",
        "                # ============================================================\n",
        "\n",
        "                temp_model = deepcopy(self.model)\n",
        "                data_batch_1 = get_data_batch(\n",
        "                    self.trainloader, self.iter_trainloader, self.device\n",
        "                )\n",
        "                grads = self.compute_grad(temp_model, data_batch_1)\n",
        "\n",
        "                for param, grad in zip(temp_model.parameters(), grads):\n",
        "                    param.data.sub_(self.alpha * grad)\n",
        "\n",
        "                data_batch_2 = get_data_batch(\n",
        "                    self.trainloader, self.iter_trainloader, self.device\n",
        "                )\n",
        "                grads = self.compute_grad(temp_model, data_batch_2)\n",
        "\n",
        "                for param, grad in zip(self.model.parameters(), grads):\n",
        "                    param.data.sub_(self.beta * grad)\n",
        "\n",
        "    def compute_grad(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        data_batch: Tuple[torch.Tensor, torch.Tensor],\n",
        "        v: Union[Tuple[torch.Tensor, ...], None] = None,\n",
        "        second_order_grads=False,\n",
        "    ):\n",
        "        x, y = data_batch\n",
        "        if second_order_grads:\n",
        "            frz_model_params = deepcopy(model.state_dict())\n",
        "            delta = 1e-3\n",
        "            dummy_model_params_1 = OrderedDict()\n",
        "            dummy_model_params_2 = OrderedDict()\n",
        "            with torch.no_grad():\n",
        "                for (layer_name, param), grad in zip(model.named_parameters(), v):\n",
        "                    dummy_model_params_1.update({layer_name: param + delta * grad})\n",
        "                    dummy_model_params_2.update({layer_name: param - delta * grad})\n",
        "\n",
        "            model.load_state_dict(dummy_model_params_1, strict=False)\n",
        "            logit_1 = model(x)\n",
        "            # loss_1 = self.criterion(logit_1, y) / y.size(-1)\n",
        "            loss_1 = self.criterion(logit_1, y)\n",
        "            grads_1 = torch.autograd.grad(loss_1, model.parameters())\n",
        "\n",
        "            model.load_state_dict(dummy_model_params_2, strict=False)\n",
        "            logit_2 = model(x)\n",
        "            loss_2 = self.criterion(logit_2, y)\n",
        "            # loss_2 = self.criterion(logit_2, y) / y.size(-1)\n",
        "            grads_2 = torch.autograd.grad(loss_2, model.parameters())\n",
        "\n",
        "            model.load_state_dict(frz_model_params)\n",
        "\n",
        "            grads = []\n",
        "            with torch.no_grad():\n",
        "                for g1, g2 in zip(grads_1, grads_2):\n",
        "                    grads.append((g1 - g2) / (2 * delta))\n",
        "            return grads\n",
        "\n",
        "        else:\n",
        "            logit = model(x)\n",
        "            # loss = self.criterion(logit, y) / y.size(-1)\n",
        "            loss = self.criterion(logit, y)\n",
        "            grads = torch.autograd.grad(loss, model.parameters())\n",
        "            return grads\n",
        "\n",
        "    def pers_N_eval(self, global_model: torch.nn.Module, pers_epochs: int):\n",
        "        self.model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "        loss_before, acc_before = eval(\n",
        "            self.model, self.valloader, self.criterion, self.device\n",
        "        )\n",
        "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.alpha)\n",
        "        for _ in range(pers_epochs):\n",
        "            x, y = get_data_batch(\n",
        "                self.trainloader, self.iter_trainloader, self.device\n",
        "            )\n",
        "            logit = self.model(x)\n",
        "            # loss = self.criterion(logit, y) / y.size(-1)\n",
        "            loss = self.criterion(logit, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        loss_after, acc_after = eval(\n",
        "            self.model, self.valloader, self.criterion, self.device\n",
        "        )\n",
        "        self.logger.log(\n",
        "            \"client [{}] [red]loss: {:.4f} -> {:.4f}   [blue]acc: {:.2f}% -> {:.2f}%\".format(\n",
        "                self.id, loss_before, loss_after, acc_before * 100.0, acc_after * 100.0,\n",
        "            )\n",
        "        )\n",
        "        return {\n",
        "            \"loss_before\": loss_before,\n",
        "            \"acc_before\": acc_before,\n",
        "            \"loss_after\": loss_after,\n",
        "            \"acc_after\": acc_after,\n",
        "        }\n",
        "    \n",
        "    # We need to finish it tomorrow\n",
        "    def client_crossmix(self, paired_id):\n",
        "        '''\n",
        "        pair_trainloader: x2q, y2q \n",
        "        pair_valloader:   x2s, y2s\n",
        "        '''\n",
        "        pair_trainloader, pair_valloader = get_dataloader(\n",
        "            self.dataset, paired_id, self.batch_size, self.valset_ratio\n",
        "        )\n",
        "        iter_pair_train = iter(pair_trainloader)\n",
        "        iter_ori_train  = iter(self.o_trainloader)\n",
        "\n",
        "        lam_mix = self.dist.sample().to(\"cuda\") # lam_mix = 1 -> mixed_representation = x2s\n",
        "\n",
        "        # processing on different batches and then concatenate them\n",
        "        for _ in range(args.metatrain_iterations):\n",
        "            x_ori_batch  = next(iter_pair_train)    \n",
        "            x_pair_batch = next(iter_ori_train)        \n",
        "\n",
        "            task_2_shuffle_id = np.arange(self.num_classes)\n",
        "            np.random.shuffle(task_2_shuffle_id)\n",
        "            task_2_shuffle_id_s = np.array(\n",
        "                [np.arange(self.batch_size) + task_2_shuffle_id[idx] * self.batch_size for idx in\n",
        "                range(self.num_classes)]).flatten()\n",
        "\n",
        "            x_pair = x_pair[task_2_shuffle_id_s]\n",
        "\n",
        "        self.cr_trainloader, _ = self.mixup_data(x_ori, x_pair, lam_mix)\n"
      ],
      "metadata": {
        "id": "2C-P1DoMPaba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MAIN"
      ],
      "metadata": {
        "id": "aJfMYbHSTY_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pairing_client(client_set):\n",
        "    np.random.shuffle(client_set)\n",
        "    client_set_1 = client_set[0:int(len(client_set)/2)]\n",
        "    client_set_2 = client_set[int(len(client_set)/2):len(client_set)]\n",
        "    selected_client_pairs = np.stack((client_set_1, client_set_2), axis=1)\n",
        "    return selected_client_pairs\n"
      ],
      "metadata": {
        "id": "6DWUkDW5Af21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    fix_random_seed(args.seed)\n",
        "    if os.path.isdir(\"./log\") == False:\n",
        "        os.mkdir(\"./log\")\n",
        "    if args.gpu and torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    global_model = get_model(args.dataset, device)\n",
        "    logger = Console(record=args.log)\n",
        "    #logger.log(f\"Arguments:\", dict(args._get_kwargs()))\n",
        "    clients_4_training, clients_4_eval, client_num_in_total = get_client_id_indices(\n",
        "        args.dataset\n",
        "    )\n",
        "\n",
        "    # init clients \n",
        "    clients = [\n",
        "        PerFedAvgClient(\n",
        "            client_id=client_id,\n",
        "            alpha=args.alpha,\n",
        "            beta=args.beta,\n",
        "            global_model=global_model,\n",
        "            criterion=torch.nn.CrossEntropyLoss(),\n",
        "            batch_size=args.batch_size,\n",
        "            dataset=args.dataset,\n",
        "            local_epochs=args.local_epochs,\n",
        "            valset_ratio=args.valset_ratio,\n",
        "            logger=logger,\n",
        "            gpu=args.gpu,\n",
        "        )\n",
        "        for client_id in range(client_num_in_total)\n",
        "    ]\n",
        "    # training\n",
        "    #logger.log(\"=\" * 20, \"TRAINING\", \"=\" * 20, style=\"bold red\")\n",
        "    for _ in track(\n",
        "        range(args.global_epochs), \"Training...\", console=logger, disable=args.log\n",
        "    ):\n",
        "        # select clients\n",
        "        selected_clients = random.sample(clients_4_training, args.client_num_per_round)\n",
        "        selected_client_pairs = pairing_client(selected_clients)\n",
        "\n",
        "        model_params_cache = []\n",
        "        # client local training\n",
        "        for client_pair in selected_client_pairs:\n",
        "            for client_idx, client_id in enumerate(client_pair):\n",
        "                if args.mix:\n",
        "                    mix_c = random.randint(0,3)   # we apply interpolation with prob 66.67%\n",
        "                    if mix_c < 3:  # interpolation\n",
        "                        # giving out interpolated representation -> self.trainloader\n",
        "                        clients[client_id].client_crossmix(client_pair[int((client_idx+1) % 2)])\n",
        "                        serialized_model_params = clients[client_id].train(\n",
        "                            global_model=global_model,\n",
        "                            hessian_free=args.hf,\n",
        "                            eval_while_training=args.eval_while_training,\n",
        "                        )                        \n",
        "                    else:          # non-interpolation\n",
        "                        # giving out interpolated representation -> self.trainloader\n",
        "                        clients[client_id].client_crossmix(client_id)\n",
        "                        serialized_model_params = clients[client_id].train(\n",
        "                            global_model=global_model,\n",
        "                            hessian_free=args.hf,\n",
        "                            eval_while_training=args.eval_while_training,\n",
        "                        )\n",
        "                else:   # normal forward for both clients\n",
        "                    serialized_model_params = clients[client_id].train(\n",
        "                        global_model=global_model,\n",
        "                        hessian_free=args.hf,\n",
        "                        eval_while_training=args.eval_while_training,\n",
        "                    )\n",
        "                model_params_cache.append(serialized_model_params)\n",
        "\n",
        "        # aggregate model parameters\n",
        "        aggregated_model_params = Aggregators.fedavg_aggregate(model_params_cache)\n",
        "        SerializationTool.deserialize_model(global_model, aggregated_model_params)\n",
        "        #logger.log(\"=\" * 60)\n",
        "    # eval\n",
        "    pers_epochs = args.local_epochs if args.pers_epochs == -1 else args.pers_epochs\n",
        "    #logger.log(\"=\" * 20, \"EVALUATION\", \"=\" * 20, style=\"bold blue\")\n",
        "    loss_before = []\n",
        "    loss_after = []\n",
        "    acc_before = []\n",
        "    acc_after = []\n",
        "    for client_id in track(\n",
        "        clients_4_eval, \"Evaluating...\", console=logger, disable=args.log\n",
        "    ):\n",
        "        stats = clients[client_id].pers_N_eval(\n",
        "            global_model=global_model, pers_epochs=pers_epochs,\n",
        "        )\n",
        "        loss_before.append(stats[\"loss_before\"])\n",
        "        loss_after.append(stats[\"loss_after\"])\n",
        "        acc_before.append(stats[\"acc_before\"])\n",
        "        acc_after.append(stats[\"acc_after\"])\n",
        "\n",
        "    #logger.log(\"=\" * 20, \"RESULTS\", \"=\" * 20, style=\"bold green\")\n",
        "    #logger.log(f\"loss_before_pers: {(sum(loss_before) / len(loss_before)):.4f}\")\n",
        "    #logger.log(f\"acc_before_pers: {(sum(acc_before) * 100.0 / len(acc_before)):.2f}%\")\n",
        "    #logger.log(f\"loss_after_pers: {(sum(loss_after) / len(loss_after)):.4f}\")\n",
        "    #logger.log(f\"acc_after_pers: {(sum(acc_after) * 100.0 / len(acc_after)):.2f}%\")\n",
        "\n",
        "    if args.log:\n",
        "        algo = \"HF\" if args.hf else \"FO\"\n",
        "        # logger.save_html(\n",
        "        #     f\"./log/{args.dataset}_{args.client_num_per_round}_{args.global_epochs}_{pers_epochs}_{algo}.html\"\n",
        "        # )"
      ],
      "metadata": {
        "id": "FM-3kT5QTaDu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380,
          "referenced_widgets": [
            "a8f04b84ee854c8dabfc7afa63f7f027",
            "06e1bc462bda41db957aa7775f3123ab"
          ]
        },
        "outputId": "3eeadec1-63cf-4779-c146-fd6638680fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8f04b84ee854c8dabfc7afa63f7f027"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bc3f31a32266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmix_c\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                         \u001b[0;31m# giving out interpolated representation -> self.trainloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                         \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_crossmix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                         serialized_model_params = clients[client_id].train(\n\u001b[1;32m     53\u001b[0m                             \u001b[0mglobal_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-60d8a0c51521>\u001b[0m in \u001b[0;36mclient_crossmix\u001b[0;34m(self, paired_id)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# processing on different batches and then concatenate them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetatrain_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mlam_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'get_args' object has no attribute 'metatrain_iterations'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "\n",
        "class RainbowMNIST(Dataset):\n",
        "\n",
        "    def __init__(self, args, mode):\n",
        "        super(RainbowMNIST, self).__init__()\n",
        "        self.args = args\n",
        "        self.nb_classes = args.num_classes\n",
        "        self.nb_samples_per_class = args.update_batch_size + args.update_batch_size_eval\n",
        "        self.n_way = args.num_classes  # n-way\n",
        "        self.k_shot = args.update_batch_size  # k-shot\n",
        "        self.k_query = args.update_batch_size_eval  # for evaluation\n",
        "        self.set_size = self.n_way * self.k_shot  # num of samples per set\n",
        "        self.query_size = self.n_way * self.k_query  # number of samples per set for evaluation\n",
        "        self.mode = mode\n",
        "        self.data_file = '{}RainbowMNIST/rainbowmnist_all.pkl'.format(args.datadir)\n",
        "\n",
        "        self.data = pickle.load(open(self.data_file, 'rb'))\n",
        "\n",
        "        self.num_groupid = len(self.data.keys())\n",
        "\n",
        "        for group_id in range(self.num_groupid):\n",
        "            self.data[group_id]['labels'] = self.data[group_id]['labels'].reshape(10, 100)[:, :20]\n",
        "            self.data[group_id]['images'] = self.data[group_id]['images'].reshape(10, 100, 28, 28, 3)[:, :20, ...]\n",
        "            self.data[group_id]['images'] = torch.tensor(np.transpose(self.data[group_id]['images'], (0, 1, 4, 2, 3)))\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.sel_group_id = np.array([49,  8, 19, 47, 25, 27, 42, 50, 24, 40,  3, 45,  6, 41,  2, 17, 14,\n",
        "              10,  5, 26, 12, 33,  9, 11, 32, 54, 28,  7, 39, 51, 46, 44, 30, 13,\n",
        "              18,  0, 34, 43, 52, 29])\n",
        "            num_of_tasks = self.sel_group_id.shape[0]\n",
        "            if self.args.ratio<1.0:\n",
        "                num_of_tasks = int(num_of_tasks*self.args.ratio)\n",
        "                self.sel_group_id = self.sel_group_id[:num_of_tasks]\n",
        "        elif self.mode == 'val':\n",
        "            self.sel_group_id = np.array([15, 16, 38, 36, 37,  4])\n",
        "        elif self.mode == 'test':\n",
        "            self.sel_group_id = np.array([35, 48, 23, 20, 22, 55,  1, 21, 31, 53])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.args.metatrain_iterations*self.args.meta_batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        self.classes_idx = np.arange(self.data[0]['images'].shape[0])\n",
        "        self.samples_idx = np.arange(self.data[0]['images'].shape[1])\n",
        "\n",
        "        support_x = torch.FloatTensor(torch.zeros((self.args.meta_batch_size, self.set_size, 3, 28, 28)))\n",
        "        query_x   = torch.FloatTensor(torch.zeros((self.args.meta_batch_size, self.query_size, 3, 28, 28)))\n",
        "\n",
        "        support_y = np.zeros([self.args.meta_batch_size, self.set_size])\n",
        "        query_y   = np.zeros([self.args.meta_batch_size, self.query_size])\n",
        "\n",
        "        for meta_batch_id in range(self.args.meta_batch_size):\n",
        "            self.choose_group = np.random.choice(self.sel_group_id, size=1, replace=False).item()\n",
        "            for j in range(10):\n",
        "                np.random.shuffle(self.samples_idx)\n",
        "                choose_samples = self.samples_idx[:self.nb_samples_per_class]\n",
        "                support_x[meta_batch_id][j * self.k_shot:(j + 1) * self.k_shot] = self.data[self.choose_group]['images'][j, choose_samples[:self.k_shot], ...]\n",
        "                query_x[meta_batch_id][j * self.k_query:(j + 1) * self.k_query] = self.data[self.choose_group]['images'][j, choose_samples[self.k_shot:], ...]\n",
        "                support_y[meta_batch_id][j * self.k_shot:(j + 1) * self.k_shot] = j\n",
        "                query_y[meta_batch_id][j * self.k_query:(j + 1) * self.k_query] = j\n",
        "\n",
        "        return support_x, torch.LongTensor(support_y), query_x, torch.LongTensor(query_y)"
      ],
      "metadata": {
        "id": "Yu3rDBu1TSFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class arg_mlti():\n",
        "    def __init__(self):\n",
        "        self.datasource = 'rainbowmnist'\n",
        "        self.num_classes = 10\n",
        "        self.num_test_task = 600\n",
        "        self.test_epoch = -1\n",
        "        self.metatrain_iterations = 15000\n",
        "        self.meta_batch_size = 25\n",
        "        self.meta_lr = 0.001\n",
        "        self.update_batch_size = 5\n",
        "        self.update_batch_size_eval = 15\n",
        "        self.num_filters = 64\n",
        "        self.weight_decay = 0.0\n",
        "        self.logdir = 'xxx'\n",
        "        self.datadir = 'xxx'\n",
        "        self.resume = 0\n",
        "        self.train = 1\n",
        "        self.mix = 0\n",
        "        self.trial = 0\n",
        "        self.ratio = 1.0\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/Shareddrives/Duong-DatDeakin/Personalized_FedAvg'\n",
        "\n",
        "args = arg_mlti()\n",
        "\n",
        "dataloader = RainbowMNIST(args, 'train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyI1aTjdWmPI",
        "outputId": "9e20ba6d-acfa-40b2-f584-14524cdd16d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/Shareddrives/Duong-DatDeakin/Personalized_FedAvg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.distributions import Beta\n",
        "\n",
        "class MLTI():\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.dist = Beta(torch.FloatTensor([2]), torch.FloatTensor([2]))\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[2]\n",
        "        H = size[3]\n",
        "        cut_rat = np.sqrt(1. - lam.cpu())\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "\n",
        "        # uniform\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "    def mixup_data(self, xs, xq, lam):\n",
        "        mixed_x = xq.clone()\n",
        "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(xq.size(), lam)\n",
        "\n",
        "        mixed_x[:, :, bbx1:bbx2, bby1:bby2] = xs[:, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (xq.size()[-1] * xq.size()[-2]))\n",
        "\n",
        "        return mixed_x, lam\n",
        "\n",
        "    def forward_crossmix(self, x1s, y1s, x1q, y1q, x2s, y2s, x2q, y2q):\n",
        "        lam_mix = self.dist.sample().to(\"cuda\")\n",
        "\n",
        "        task_2_shuffle_id = np.arange(self.args.num_classes)\n",
        "        np.random.shuffle(task_2_shuffle_id)\n",
        "        task_2_shuffle_id_s = np.array(\n",
        "            [np.arange(self.args.update_batch_size) + task_2_shuffle_id[idx] * self.args.update_batch_size for idx in\n",
        "                range(self.args.num_classes)]).flatten()\n",
        "        # task_2_shuffle_id_q = np.array(\n",
        "        #     [np.arange(self.args.update_batch_size_eval) + task_2_shuffle_id[idx] * self.args.update_batch_size_eval for\n",
        "        #      idx in range(self.args.num_classes)]).flatten()\n",
        "\n",
        "        # x2s = x2s[task_2_shuffle_id_s]\n",
        "\n",
        "        print(f\"shuffle s: {len(task_2_shuffle_id_s)} - {task_2_shuffle_id_s}\")\n",
        "        # print(f\"shuffle q: {len(task_2_shuffle_id_q)} - {task_2_shuffle_id_q}\")\n",
        "\n",
        "        x2s_s = x2s[task_2_shuffle_id_s]\n",
        "\n",
        "        x_mix_s, _ = self.mixup_data(x1s, x2s_s, torch.tensor([1]).to(\"cuda\"))\n",
        "        \n",
        "        for id in range(len(task_2_shuffle_id_s)):\n",
        "            if id == 3: \n",
        "                break\n",
        "            print(f\"ID: {id}\")\n",
        "            print(np.linalg.norm(x2s_s[id]-x2s[id]))\n",
        "            print(np.linalg.norm(x_mix_s[id]-x2s[id]))\n",
        "            print(np.linalg.norm(x_mix_s[id]-x2s_s[id]))\n",
        "\n",
        "learner = MLTI(args)\n",
        "\n",
        "for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(dataloader):\n",
        "    if step == 1:\n",
        "        break\n",
        "    # print(y_spt[0])\n",
        "    # print(y_spt[1])\n",
        "    learner.forward_crossmix(x_spt[0], y_spt[0],\n",
        "                             x_qry[0], y_qry[0],\n",
        "                             x_spt[1], y_spt[1],\n",
        "                             x_qry[1], y_qry[1]\n",
        "                            )"
      ],
      "metadata": {
        "id": "ndfdV6a_fb0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dde5f5d-2f66-40aa-d95a-b9356e72eb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shuffle s: 50 - [35 36 37 38 39 40 41 42 43 44 20 21 22 23 24 45 46 47 48 49  5  6  7  8\n",
            "  9  0  1  2  3  4 15 16 17 18 19 30 31 32 33 34 25 26 27 28 29 10 11 12\n",
            " 13 14]\n",
            "ID: 0\n",
            "20.396078\n",
            "20.396078\n",
            "0.0\n",
            "ID: 1\n",
            "19.026299\n",
            "19.026299\n",
            "0.0\n",
            "ID: 2\n",
            "17.378147\n",
            "17.378147\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-221ae9303c7e>:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  cut_w = np.int(W * cut_rat)\n",
            "<ipython-input-43-221ae9303c7e>:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  cut_h = np.int(H * cut_rat)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_spt[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHLp6aGyCMD0",
        "outputId": "f2f4c9c0-d90c-4178-f418-35902e2994e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52\n"
          ]
        }
      ]
    }
  ]
}