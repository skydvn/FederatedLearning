{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd98169ebeeb40bea353a3045ee62886": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4026b899edc540459973a2d1e769fbb9",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Training... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "4026b899edc540459973a2d1e769fbb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b1ac056cd364a6a831b5f5952de9f0a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_5eaaf327bbbc40df9e1263a7496e6df5",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Evaluating... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "5eaaf327bbbc40df9e1263a7496e6df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmzufljHWo4A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "from typing import Iterator, Tuple, Union, Dict, List\n",
        "from argparse import ArgumentParser, Namespace\n",
        "from collections import OrderedDict, Counter\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, np.nan, 4])\n",
        "print(x)\n",
        "# x!=x\n",
        "# print(x)\n",
        "torch.isnan(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYY60xeAMM6J",
        "outputId": "032fc0a9-34cc-4a04-a6b5-b8d999b8e85d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., nan, 4.])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False,  True, False])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fedlab~=1.1.4\n",
        "from fedlab.utils.serialization import SerializationTool\n",
        "from fedlab.utils.aggregator import Aggregators\n",
        "from fedlab.utils.dataset.slicing import noniid_slicing"
      ],
      "metadata": {
        "id": "MHgGoPWjX4oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5f4140-aff7-4f6c-d233-3c78c605c599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fedlab~=1.1.4\n",
            "  Downloading fedlab-1.1.5.tar.gz (36 kB)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (0.14.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (1.3.5)\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fedlab~=1.1.4) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.1->fedlab~=1.1.4) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.8.2->fedlab~=1.1.4) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.8.2->fedlab~=1.1.4) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fedlab~=1.1.4) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fedlab~=1.1.4) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->fedlab~=1.1.4) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->fedlab~=1.1.4) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->fedlab~=1.1.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->fedlab~=1.1.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->fedlab~=1.1.4) (1.24.3)\n",
            "Building wheels for collected packages: fedlab\n",
            "  Building wheel for fedlab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fedlab: filename=fedlab-1.1.5-py3-none-any.whl size=56049 sha256=86ddf05af96f4bead5b8fcf4a5c6823fcd23d551b18bdad37e322c97a1f34223\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/fa/bf/095e6b35c4cfcfe0eed549aea1a2b280bc7617f86c6e165810\n",
            "Successfully built fedlab\n",
            "Installing collected packages: pynvml, fedlab\n",
            "Successfully installed fedlab-1.1.5 pynvml-11.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rich\n",
        "import rich\n",
        "from rich.console import Console\n",
        "from rich.progress import track"
      ],
      "metadata": {
        "id": "fe-Ub3lGYBit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7b1263-e257-4fc9-8dae-cebe48165907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rich\n",
            "  Downloading rich-13.0.0-py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich) (4.4.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: commonmark, rich\n",
            "Successfully installed commonmark-0.9.1 rich-13.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install path\n",
        "from path import Path"
      ],
      "metadata": {
        "id": "VnmJGjICYteL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec71d01-2824-411d-dcbf-60c4b74a404f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting path\n",
            "  Downloading path-16.6.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: path\n",
            "Successfully installed path-16.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MOUNT"
      ],
      "metadata": {
        "id": "x_hAP6OFyzLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "PQpR9mnAy1HS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51fd2e3-ffc7-4368-9ac7-8d33351cb398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/Shareddrives/Duong-Binh-BKHN/Simt_perFedAvg'"
      ],
      "metadata": {
        "id": "1USkYfo1zDDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb1c842-02d5-4ba7-94b3-b4f1e7e30abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Duong-Binh-BKHN/Simt_perFedAvg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "o_LO2k2iz6Xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65f521a-7841-41ec-dc82-97cb60576cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Duong-Binh-BKHN/Simt_perFedAvg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA/DATASET"
      ],
      "metadata": {
        "id": "Ms1kWZKeY2Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        subset=None,\n",
        "        data=None,\n",
        "        targets=None,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "    ) -> None:\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        if (data is not None) and (targets is not None):\n",
        "            self.data = data.unsqueeze(1)\n",
        "            self.targets = targets\n",
        "        elif subset is not None:\n",
        "            self.data = torch.stack(\n",
        "                list(\n",
        "                    map(\n",
        "                        lambda tup: tup[0]\n",
        "                        if isinstance(tup[0], torch.Tensor)\n",
        "                        else torch.tensor(tup[0]),\n",
        "                        subset,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            self.targets = torch.stack(\n",
        "                list(\n",
        "                    map(\n",
        "                        lambda tup: tup[1]\n",
        "                        if isinstance(tup[1], torch.Tensor)\n",
        "                        else torch.tensor(tup[1]),\n",
        "                        subset,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Data Format: subset: Tuple(data: Tensor / Image / np.ndarray, targets: Tensor) OR data: List[Tensor]  targets: List[Tensor]\"\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data, targets = self.data[index], self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            data = self.transform(self.data[index])\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            targets = self.target_transform(self.targets[index])\n",
        "\n",
        "        return data, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "\n",
        "class CIFARDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        subset=None,\n",
        "        data=None,\n",
        "        targets=None,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "    ) -> None:\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        if (data is not None) and (targets is not None):\n",
        "            self.data = data.unsqueeze(1)\n",
        "            self.targets = targets\n",
        "        elif subset is not None:\n",
        "            self.data = torch.stack(\n",
        "                list(\n",
        "                    map(\n",
        "                        lambda tup: tup[0]\n",
        "                        if isinstance(tup[0], torch.Tensor)\n",
        "                        else torch.tensor(tup[0]),\n",
        "                        subset,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            self.targets = torch.stack(\n",
        "                list(\n",
        "                    map(\n",
        "                        lambda tup: tup[1]\n",
        "                        if isinstance(tup[1], torch.Tensor)\n",
        "                        else torch.tensor(tup[1]),\n",
        "                        subset,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Data Format: subset: Tuple(data: Tensor / Image / np.ndarray, targets: Tensor) OR data: List[Tensor]  targets: List[Tensor]\"\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, targets = self.data[index], self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(self.data[index])\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            targets = self.target_transform(self.targets[index])\n",
        "\n",
        "        return img, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n"
      ],
      "metadata": {
        "id": "OGJ04z7GY7nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PREPROCESS"
      ],
      "metadata": {
        "id": "taZ4Ge-SJVqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CURRENT_DIR = Path(__file__).parent.abspath()\n",
        "CURRENT_DIR = \"/content/drive/Shareddrives/Duong-Binh-BKHN/Simt_perFedAvg\"\n",
        "\n",
        "DATASET = {\n",
        "    \"mnist\": (MNIST, MNISTDataset),\n",
        "    \"cifar\": (CIFAR10, CIFARDataset),\n",
        "}\n",
        "\n",
        "MEAN = {\n",
        "    \"mnist\": (0.1307,),\n",
        "    \"cifar\": (0.4914, 0.4822, 0.4465),\n",
        "}\n",
        "\n",
        "STD = {\n",
        "    \"mnist\": (0.3015,),\n",
        "    \"cifar\": (0.2023, 0.1994, 0.2010),\n",
        "}\n",
        "\n",
        "def preprocess(args: Namespace) -> None:\n",
        "    dataset_dir = CURRENT_DIR + \"/\" + args.dataset\n",
        "    pickles_dir = CURRENT_DIR + \"/\" + args.dataset + \"/\" + \"pickles\"\n",
        "\n",
        "    np.random.seed(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    num_train_clients = int(args.client_num_in_total * args.fraction)\n",
        "    num_test_clients = args.client_num_in_total - num_train_clients\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Normalize(MEAN[args.dataset], STD[args.dataset]),]\n",
        "    )\n",
        "    target_transform = None\n",
        "    trainset_stats = {}\n",
        "    testset_stats = {}\n",
        "\n",
        "    if not os.path.isdir(CURRENT_DIR + \"/\" + args.dataset):\n",
        "        os.mkdir(CURRENT_DIR + \"/\" + args.dataset)\n",
        "    if os.path.isdir(pickles_dir):\n",
        "        os.system(f\"rm -rf {pickles_dir}\")\n",
        "    os.mkdir(f\"{pickles_dir}\")\n",
        "\n",
        "    ori_dataset, target_dataset = DATASET[args.dataset]\n",
        "    trainset = ori_dataset(\n",
        "        dataset_dir, train=True, download=True, transform=transforms.ToTensor()\n",
        "    )\n",
        "    testset = ori_dataset(dataset_dir, train=False, transform=transforms.ToTensor())\n",
        "\n",
        "    num_classes = 10 if args.classes <= 0 else args.classes\n",
        "    all_trainsets, trainset_stats = randomly_alloc_classes(\n",
        "        ori_dataset=trainset,\n",
        "        target_dataset=target_dataset,\n",
        "        num_clients=num_train_clients,\n",
        "        num_classes=num_classes,\n",
        "        transform=transform,\n",
        "        target_transform=target_transform,\n",
        "    )\n",
        "    all_testsets, testset_stats = randomly_alloc_classes(\n",
        "        ori_dataset=testset,\n",
        "        target_dataset=target_dataset,\n",
        "        num_clients=num_test_clients,\n",
        "        num_classes=num_classes,\n",
        "        transform=transform,\n",
        "        target_transform=target_transform,\n",
        "    )\n",
        "\n",
        "    all_datasets = all_trainsets + all_testsets\n",
        "\n",
        "    for client_id, dataset in enumerate(all_datasets):\n",
        "        with open(pickles_dir + \"/\" + str(client_id) + \".pkl\", \"wb\") as f:\n",
        "            pickle.dump(dataset, f)\n",
        "    with open(pickles_dir + \"/\" + \"seperation.pkl\", \"wb\") as f:\n",
        "        pickle.dump(\n",
        "            {\n",
        "                \"train\": [i for i in range(num_train_clients)],\n",
        "                \"test\": [i for i in range(num_train_clients, args.client_num_in_total)],\n",
        "                \"total\": args.client_num_in_total,\n",
        "            },\n",
        "            f,\n",
        "        )\n",
        "    with open(dataset_dir + \"/\" + \"all_stats.json\", \"w\") as f:\n",
        "        json.dump({\"train\": trainset_stats, \"test\": testset_stats}, f)\n",
        "\n",
        "def randomly_alloc_classes(\n",
        "    ori_dataset: Dataset,\n",
        "    target_dataset: Dataset,\n",
        "    num_clients: int,\n",
        "    num_classes: int,\n",
        "    transform=None,\n",
        "    target_transform=None,\n",
        ") -> Tuple[List[Dataset], Dict[str, Dict[str, int]]]:\n",
        "    dict_users = noniid_slicing(ori_dataset, num_clients, num_clients * num_classes)\n",
        "    stats = {}\n",
        "    for i, indices in dict_users.items():\n",
        "        targets_numpy = np.array(ori_dataset.targets)\n",
        "        stats[f\"client {i}\"] = {\"x\": 0, \"y\": {}}\n",
        "        stats[f\"client {i}\"][\"x\"] = len(indices)\n",
        "        stats[f\"client {i}\"][\"y\"] = Counter(targets_numpy[indices].tolist())\n",
        "    datasets = []\n",
        "    for indices in dict_users.values():\n",
        "        datasets.append(\n",
        "            target_dataset(\n",
        "                [ori_dataset[i] for i in indices],\n",
        "                transform=transform,\n",
        "                target_transform=target_transform,\n",
        "            )\n",
        "        )\n",
        "    return datasets, stats\n",
        "\n",
        "class get_args_preprocess():\n",
        "  def __init__(self):\n",
        "    self.dataset = \"mnist\"\n",
        "    self.client_num_in_total = 200\n",
        "    self.fraction = 0.9\n",
        "    self.classes = 2\n",
        "    self.seed = 0\n",
        "\n",
        "args = get_args_preprocess()\n",
        "preprocess(args)"
      ],
      "metadata": {
        "id": "2dU0ag3wJY-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "546389d3-0c88-421a-bcca-fb8bd0fc9ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fedlab/utils/dataset/slicing.py:38: UserWarning: warning: the length of dataset isn't divided exactly by num_shard.some samples will be dropped.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA/UTILS"
      ],
      "metadata": {
        "id": "UJKhRhvnQHU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DICT = {\n",
        "    \"mnist\": MNISTDataset,\n",
        "    \"cifar\": CIFARDataset,\n",
        "}\n",
        "#CURRENT_DIR = Path(__file__).parent.abspath()\n",
        "CURRENT_DIR = \"/content/drive/Shareddrives/Duong-Binh-BKHN/Simt_perFedAvg\"\n",
        "\n",
        "def get_dataloader(dataset: str, client_id: int, batch_size=20, valset_ratio=0.1):\n",
        "    pickles_dir = CURRENT_DIR + \"/\" + dataset + \"/\" \"pickles\"\n",
        "    if os.path.isdir(pickles_dir) is False:\n",
        "        raise RuntimeError(\"Please preprocess and create pickles first.\")\n",
        "\n",
        "    with open(pickles_dir + \"/\" + str(client_id) + \".pkl\", \"rb\") as f:\n",
        "        client_dataset: DATASET_DICT[dataset] = pickle.load(f)\n",
        "\n",
        "    val_num_samples = int(valset_ratio * len(client_dataset))\n",
        "    train_num_samples = len(client_dataset) - val_num_samples\n",
        "\n",
        "    trainset, valset = random_split(\n",
        "        client_dataset, [train_num_samples, val_num_samples]\n",
        "    )\n",
        "    trainloader = DataLoader(trainset, batch_size, drop_last=True)\n",
        "    valloader = DataLoader(valset, batch_size)\n",
        "\n",
        "    return trainloader, valloader\n",
        "\n",
        "def get_client_id_indices(dataset):\n",
        "    dataset_pickles_path = CURRENT_DIR + \"/\" + dataset + \"/\" + \"pickles\"\n",
        "    with open(dataset_pickles_path + \"/\" + \"seperation.pkl\", \"rb\") as f:\n",
        "        seperation = pickle.load(f)\n",
        "    return (seperation[\"train\"], seperation[\"test\"], seperation[\"total\"])"
      ],
      "metadata": {
        "id": "_buZQAOvYP0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MODEL"
      ],
      "metadata": {
        "id": "8d1jlnQ9OuBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class elu(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(elu, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.where(x >= 0, x, 0.2 * (torch.exp(x) - 1))\n",
        "\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self, in_c, out_c) -> None:\n",
        "        super(linear, self).__init__()\n",
        "        self.w = nn.Parameter(\n",
        "            torch.randn(out_c, in_c) * torch.sqrt(torch.tensor(2 / in_c))\n",
        "        )\n",
        "        self.b = nn.Parameter(torch.randn(out_c))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.w, self.b)\n",
        "\n",
        "\n",
        "class MLP_MNIST(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(MLP_MNIST, self).__init__()\n",
        "        self.fc1 = linear(28 * 28, 80)\n",
        "        self.fc2 = linear(80, 60)\n",
        "        self.fc3 = linear(60, 10)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.activation = elu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP_CIFAR10(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(MLP_CIFAR10, self).__init__()\n",
        "        self.fc1 = linear(32 * 32 * 3, 80)\n",
        "        self.fc2 = linear(80, 60)\n",
        "        self.fc3 = linear(60, 10)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.activation = elu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "MODEL_DICT = {\"mnist\": MLP_MNIST, \"cifar\": MLP_CIFAR10}\n",
        "\n",
        "def get_model(dataset, device):\n",
        "    return MODEL_DICT[dataset]().to(device)"
      ],
      "metadata": {
        "id": "cQ7SZ-9gOwSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###UTILS"
      ],
      "metadata": {
        "id": "UE-4H9KGQm7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class get_args():\n",
        "  def __init__(self):\n",
        "    self.alpha = 1e-2\n",
        "    self.beta = 1e-3\n",
        "    self.lmda = 0.2\n",
        "    self.global_epochs = 200\n",
        "    self.local_epochs = 4\n",
        "    self.pers_epochs = 1\n",
        "    self.hf = 1\n",
        "    self.batch_size = 40\n",
        "    self.valset_ratio = 0.1\n",
        "    self.dataset = \"mnist\"  #choices = [\"mnist\", \"cifar\"]\n",
        "    self.client_num_per_round = 10\n",
        "    self.seed = 17\n",
        "    self.gpu = 1\n",
        "    self.eval_while_training = 1\n",
        "    self.log = 0\n",
        "    \n",
        "# def get_args():\n",
        "#     parser = ArgumentParser()\n",
        "#     parser.add_argument(\"--alpha\", type=float, default=1e-2)\n",
        "#     parser.add_argument(\"--beta\", type=float, default=1e-3)\n",
        "#     parser.add_argument(\"--global_epochs\", type=int, default=200)\n",
        "#     parser.add_argument(\"--local_epochs\", type=int, default=4)\n",
        "#     parser.add_argument(\n",
        "#         \"--pers_epochs\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=\"Indicate how many data batches would be used for personalization. Negatives means that equal to train phase.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--hf\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=\"0 for performing Per-FedAvg(FO), others for Per-FedAvg(HF)\",\n",
        "#     )\n",
        "#     parser.add_argument(\"--batch_size\", type=int, default=40)\n",
        "#     parser.add_argument(\n",
        "#         \"--valset_ratio\",\n",
        "#         type=float,\n",
        "#         default=0.1,\n",
        "#         help=\"Proportion of val set in the entire client local dataset\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--dataset\", type=str, choices=[\"mnist\", \"cifar\"], default=\"mnist\"\n",
        "#     )\n",
        "#     parser.add_argument(\"--client_num_per_round\", type=int, default=10)\n",
        "#     parser.add_argument(\"--seed\", type=int, default=17)\n",
        "#     parser.add_argument(\n",
        "#         \"--gpu\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=\"Non-zero value for using gpu, 0 for using cpu\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--eval_while_training\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=\"Non-zero value for performing local evaluation before and after local training\",\n",
        "#     )\n",
        "#     parser.add_argument(\"--log\", type=int, default=0)\n",
        "#     return parser.parse_args()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: Union[torch.nn.MSELoss, torch.nn.CrossEntropyLoss],\n",
        "    device=torch.device(\"cpu\"),\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_samples = 0\n",
        "    acc = 0\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logit = model(x)\n",
        "        # total_loss += criterion(logit, y) / y.size(-1)\n",
        "        total_loss += criterion(logit, y)\n",
        "        pred = torch.softmax(logit, -1).argmax(-1)\n",
        "        acc += torch.eq(pred, y).int().sum()\n",
        "        num_samples += y.size(-1)\n",
        "    model.train()\n",
        "    return total_loss, acc / num_samples\n",
        "\n",
        "def get_data_batch(\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    iterator: Iterator,\n",
        "    device=torch.device(\"cpu\"),\n",
        "):\n",
        "    try:\n",
        "        x, y = next(iterator)\n",
        "    except StopIteration:\n",
        "        iterator = iter(dataloader)\n",
        "        x, y = next(iterator)\n",
        "\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "def fix_random_seed(seed: int):\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "xV8kxW7kQoJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PERFEDAVG"
      ],
      "metadata": {
        "id": "nAX4Nd9VPYYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerFedAvgClient:\n",
        "    def __init__(\n",
        "        self,\n",
        "        client_id: int,\n",
        "        alpha: float,\n",
        "        beta: float,\n",
        "        lmbda: float,\n",
        "        global_model: torch.nn.Module,\n",
        "        momentum_model: torch.nn.Module,\n",
        "        criterion: Union[torch.nn.CrossEntropyLoss, torch.nn.MSELoss],\n",
        "        batch_size: int,\n",
        "        dataset: str,\n",
        "        local_epochs: int,\n",
        "        valset_ratio: float,\n",
        "        logger: rich.console.Console,\n",
        "        gpu: int,\n",
        "    ):\n",
        "\n",
        "        # Set cuda \n",
        "        if gpu and torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "        self.logger = logger\n",
        "\n",
        "        self.local_epochs = local_epochs\n",
        "        self.criterion = criterion\n",
        "        self.id = client_id\n",
        "\n",
        "        self.model = deepcopy(global_model)\n",
        "        self.momentum_model = deepcopy(momentum_model)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.lamda = lmbda\n",
        "        self.trainloader, self.valloader = get_dataloader(\n",
        "            dataset, client_id, batch_size, valset_ratio\n",
        "        )\n",
        "        self.iter_trainloader = iter(self.trainloader)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        global_model: torch.nn.Module,\n",
        "        hessian_free=False,\n",
        "        eval_while_training=False,\n",
        "    ):\n",
        "        self.model.load_state_dict(global_model.state_dict())\n",
        "        if eval_while_training:\n",
        "            loss_before, acc_before = eval(\n",
        "                self.model, self.valloader, self.criterion, self.device\n",
        "            )\n",
        "        self._train(hessian_free)\n",
        "\n",
        "        if eval_while_training:\n",
        "            loss_after, acc_after = eval(\n",
        "                self.model, self.valloader, self.criterion, self.device\n",
        "            )\n",
        "            self.logger.log(\n",
        "                \"client [{}] [red]loss: {:.4f} -> {:.4f}   [blue]acc: {:.2f}% -> {:.2f}%\".format(\n",
        "                    self.id,\n",
        "                    loss_before,\n",
        "                    loss_after,\n",
        "                    acc_before * 100.0,\n",
        "                    acc_after * 100.0,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Return model parameters\n",
        "        return SerializationTool.serialize_model(self.model)\n",
        "\n",
        "    def _train(self, hessian_free=False):\n",
        "        if hessian_free:  # Per-FedAvg(HF)\n",
        "            for _ in range(self.local_epochs):\n",
        "              \n",
        "                temp_model = deepcopy(self.model)\n",
        "                temp_momentum_model = deepcopy(self.momentum_model)\n",
        "            \n",
        "                data_batch_1 = get_data_batch(\n",
        "                    self.trainloader, self.iter_trainloader, self.device\n",
        "                )\n",
        "                grads = self.compute_grad(temp_model, temp_momentum_model, data_batch_1)                # adapt (global theta, data_batch_1).           \n",
        "                for param, grad in zip(temp_model.parameters(), grads):\n",
        "                    param.data.sub_(self.alpha * grad) \n",
        "\n",
        "                # data_batch_1_ = get_data_batch(\n",
        "                #     self.trainloader, self.iter_trainloader, self.device\n",
        "                # )\n",
        "                grads_ = self.compute_grad(temp_momentum_model, temp_momentum_model, data_batch_1)      # adapt (momentum theta, data_batch_1_)        \n",
        "                for param_, grad_ in zip(temp_momentum_model.parameters(), grads_):\n",
        "                    param_.data.sub_(self.alpha * grad_)                          \n",
        "\n",
        "                # Dropout (theta_global model)\n",
        "                # phi_dropout_model=self.Dropout_func(temp_model,0.2)\n",
        "                phi_params = SerializationTool.serialize_model(temp_model)\n",
        "                m = nn.Dropout(p=0.2)\n",
        "                phi_params_output = m(phi_params)\n",
        "\n",
        "                phi_dropout_model = deepcopy(self.model)\n",
        "\n",
        "                SerializationTool.deserialize_model(phi_dropout_model, phi_params_output)\n",
        "\n",
        "                data_batch_2 = get_data_batch(\n",
        "                    self.trainloader, self.iter_trainloader, self.device\n",
        "                )\n",
        "                grads_1st = self.compute_grad(phi_dropout_model, temp_momentum_model, data_batch_2)\n",
        "                   \n",
        "                loss_teach = self.compute_grad(phi_dropout_model, temp_momentum_model, data_batch_2, KL_div=True)\n",
        "\n",
        "                data_batch_3 = get_data_batch(\n",
        "                    self.trainloader, self.iter_trainloader, self.device\n",
        "                )\n",
        "\n",
        "                grads_2nd = self.compute_grad(\n",
        "                    self.model, temp_momentum_model, data_batch_3, v=grads_1st, second_order_grads=True\n",
        "                )\n",
        "                # NOTE: Go check https://github.com/KarhouTam/Per-FedAvg/issues/2 if you confuse about the model update.\n",
        "                for param, grad2, lossteach in zip(\n",
        "                    self.model.parameters(), grads_2nd, loss_teach      # grad_total\n",
        "                ):\n",
        "                    param.data.sub_(self.beta * lossteach - self.beta * self.alpha * grad2)\n",
        "\n",
        "                # for param, grad1, grad2, grad_loss_teach in zip(\n",
        "                #     self.model.parameters(), grads_1st, grads_2nd, loss_teach          # grad_total\n",
        "                # ):\n",
        "                #     param.data.sub_(self.beta * grad1 - self.beta * self.alpha * grad2)\n",
        "\n",
        "        # else:  # Per-FedAvg(FO)\n",
        "        #     for _ in range(self.local_epochs):\n",
        "        #         # ========================== FedAvg ==========================\n",
        "        #         # NOTE: You can uncomment those codes for running FedAvg.\n",
        "        #         #       When you're trying to run FedAvg, comment other codes in this branch.\n",
        "\n",
        "        #         # data_batch = utils.get_data_batch(\n",
        "        #         #     self.trainloader, self.iter_trainloader, self.device\n",
        "        #         # )\n",
        "        #         # grads = self.compute_grad(self.model, data_batch)\n",
        "        #         # for param, grad in zip(self.model.parameters(), grads):\n",
        "        #         #     param.data.sub_(self.beta * grad)\n",
        "\n",
        "        #         # ============================================================\n",
        "\n",
        "        #         temp_model = deepcopy(self.model)\n",
        "        #         data_batch_1 = get_data_batch(\n",
        "        #             self.trainloader, self.iter_trainloader, self.device\n",
        "        #         )\n",
        "        #         grads = self.compute_grad(temp_model, data_batch_1)\n",
        "\n",
        "        #         for param, grad in zip(temp_model.parameters(), grads):\n",
        "        #             param.data.sub_(self.alpha * grad)\n",
        "\n",
        "        #         data_batch_2 = get_data_batch(\n",
        "        #             self.trainloader, self.iter_trainloader, self.device\n",
        "        #         )\n",
        "        #         grads = self.compute_grad(temp_model, data_batch_2)\n",
        "\n",
        "        #         for param, grad in zip(self.model.parameters(), grads):\n",
        "        #             param.data.sub_(self.beta * grad)\n",
        "\n",
        "    def compute_grad(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        teacher_model: torch.nn.Module,\n",
        "        data_batch: Tuple[torch.Tensor, torch.Tensor],\n",
        "        v: Union[Tuple[torch.Tensor, ...], None] = None,\n",
        "        second_order_grads=False,\n",
        "        KL_div=False,\n",
        "    ):\n",
        "        x, y = data_batch\n",
        "        if second_order_grads:\n",
        "            frz_model_params = deepcopy(model.state_dict())\n",
        "            delta = 1e-3\n",
        "            dummy_model_params_1 = OrderedDict()\n",
        "            dummy_model_params_2 = OrderedDict()\n",
        "            with torch.no_grad():\n",
        "                for (layer_name, param), grad in zip(model.named_parameters(), v):\n",
        "                    dummy_model_params_1.update({layer_name: param + delta * grad})\n",
        "                    dummy_model_params_2.update({layer_name: param - delta * grad})\n",
        "\n",
        "            model.load_state_dict(dummy_model_params_1, strict=False)\n",
        "            logit_1 = model(x)\n",
        "            # loss_1 = self.criterion(logit_1, y) / y.size(-1)\n",
        "            loss_1 = self.criterion(logit_1, y)\n",
        "            grads_1 = torch.autograd.grad(loss_1, model.parameters())\n",
        "\n",
        "            model.load_state_dict(dummy_model_params_2, strict=False)\n",
        "            logit_2 = model(x)\n",
        "            loss_2 = self.criterion(logit_2, y)\n",
        "            # loss_2 = self.criterion(logit_2, y) / y.size(-1)\n",
        "            grads_2 = torch.autograd.grad(loss_2, model.parameters())\n",
        "\n",
        "            model.load_state_dict(frz_model_params)\n",
        "\n",
        "            grads = []\n",
        "            with torch.no_grad():\n",
        "                for g1, g2 in zip(grads_1, grads_2):\n",
        "                    grads.append((g1 - g2) / (2 * delta))\n",
        "            return grads\n",
        "\n",
        "        elif KL_div:\n",
        "            output_batch = model(x)\n",
        "            label_batch = y\n",
        "            output_teacher_batch = teacher_model(x)\n",
        "            params_temp = 4\n",
        "\n",
        "            KL_loss = nn.KLDivLoss(reduction=\"batchmean\")(F.log_softmax(output_batch/params_temp, dim=1),\n",
        "                      F.softmax(output_teacher_batch/params_temp, dim=1)) * (params_temp * params_temp) * (1 - self.lamda) + \\\n",
        "                      F.cross_entropy(output_batch, label_batch) * self.lamda                                                 \n",
        "\n",
        "            grads = torch.autograd.grad(KL_loss, model.parameters())\n",
        "            \n",
        "            return grads\n",
        "\n",
        "        else:\n",
        "            logit = model(x)\n",
        "            # loss = self.criterion(logit, y) / y.size(-1)\n",
        "            loss = self.criterion(logit, y)\n",
        "            grads = torch.autograd.grad(loss, model.parameters())\n",
        "            \n",
        "            return grads\n",
        "\n",
        "\n",
        "\n",
        "    def pers_N_eval(self, global_model: torch.nn.Module, pers_epochs: int):\n",
        "        self.model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "        loss_before, acc_before = eval(\n",
        "            self.model, self.valloader, self.criterion, self.device\n",
        "        )\n",
        "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.alpha)\n",
        "        for _ in range(pers_epochs):\n",
        "            x, y = get_data_batch(\n",
        "                self.trainloader, self.iter_trainloader, self.device\n",
        "            )\n",
        "            logit = self.model(x)\n",
        "            # loss = self.criterion(logit, y) / y.size(-1)\n",
        "            loss = self.criterion(logit, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        loss_after, acc_after = eval(\n",
        "            self.model, self.valloader, self.criterion, self.device\n",
        "        )\n",
        "        self.logger.log(\n",
        "            \"client [{}] [red]loss: {:.4f} -> {:.4f}   [blue]acc: {:.2f}% -> {:.2f}%\".format(\n",
        "                self.id, loss_before, loss_after, acc_before * 100.0, acc_after * 100.0,\n",
        "            )\n",
        "        )\n",
        "        return {\n",
        "            \"loss_before\": loss_before,\n",
        "            \"acc_before\": acc_before,\n",
        "            \"loss_after\": loss_after,\n",
        "            \"acc_after\": acc_after,\n",
        "        }"
      ],
      "metadata": {
        "id": "2C-P1DoMPaba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MAIN"
      ],
      "metadata": {
        "id": "aJfMYbHSTY_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    fix_random_seed(args.seed)\n",
        "    if os.path.isdir(\"./log\") == False:\n",
        "        os.mkdir(\"./log\")\n",
        "    if args.gpu and torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    global_model = get_model(args.dataset, device)\n",
        "    momentum_model= deepcopy(global_model)\n",
        "    logger = Console(record=args.log)\n",
        "    #logger.log(f\"Arguments:\", dict(args._get_kwargs()))\n",
        "    clients_4_training, clients_4_eval, client_num_in_total = get_client_id_indices(\n",
        "        args.dataset\n",
        "    )\n",
        "\n",
        "    # init clients\n",
        "    clients = [\n",
        "        PerFedAvgClient(\n",
        "            client_id=client_id,\n",
        "            alpha=args.alpha,\n",
        "            beta=args.beta,\n",
        "            lmbda=args.lmda,\n",
        "            global_model=global_model,\n",
        "            momentum_model=momentum_model,\n",
        "            criterion=torch.nn.CrossEntropyLoss(),\n",
        "            batch_size=args.batch_size,\n",
        "            dataset=args.dataset,\n",
        "            local_epochs=args.local_epochs,\n",
        "            valset_ratio=args.valset_ratio,\n",
        "            logger=logger,\n",
        "            gpu=args.gpu,\n",
        "        )\n",
        "        for client_id in range(client_num_in_total)\n",
        "    ]\n",
        "    # training\n",
        "    #logger.log(\"=\" * 20, \"TRAINING\", \"=\" * 20, style=\"bold red\")\n",
        "    for _ in track(\n",
        "        range(args.global_epochs), \"Training...\", console=logger, disable=args.log\n",
        "    ):\n",
        "        # select clients\n",
        "        selected_clients = random.sample(clients_4_training, args.client_num_per_round)\n",
        "\n",
        "        model_params_cache = []\n",
        "        # client local training\n",
        "        for client_id in selected_clients:\n",
        "            serialized_model_params = clients[client_id].train(\n",
        "                global_model=global_model,\n",
        "                hessian_free=args.hf,\n",
        "                eval_while_training=args.eval_while_training,\n",
        "            )\n",
        "            model_params_cache.append(serialized_model_params)\n",
        "\n",
        "        # aggregate model parameters\n",
        "        aggregated_model_params = Aggregators.fedavg_aggregate(model_params_cache)\n",
        "        momemtum_params = SerializationTool.serialize_model(momentum_model)\n",
        "        momemtum_params = 0.995 * momemtum_params + (1-0.995) * aggregated_model_params\n",
        "        \n",
        "        SerializationTool.deserialize_model(global_model, aggregated_model_params)\n",
        "        SerializationTool.deserialize_model(momentum_model, momemtum_params)\n",
        "        #assign parameters to self.model\n",
        "        #logger.log(\"=\" * 60)\n",
        "    # eval\n",
        "    pers_epochs = args.local_epochs if args.pers_epochs == -1 else args.pers_epochs\n",
        "    #logger.log(\"=\" * 20, \"EVALUATION\", \"=\" * 20, style=\"bold blue\")\n",
        "    loss_before = []\n",
        "    loss_after = []\n",
        "    acc_before = []\n",
        "    acc_after = []\n",
        "    for client_id in track(\n",
        "        clients_4_eval, \"Evaluating...\", console=logger, disable=args.log\n",
        "    ):\n",
        "        stats = clients[client_id].pers_N_eval(\n",
        "            global_model=global_model, pers_epochs=pers_epochs,               #PerFedAvg\n",
        "        )\n",
        "        loss_before.append(stats[\"loss_before\"])\n",
        "        loss_after.append(stats[\"loss_after\"])\n",
        "        acc_before.append(stats[\"acc_before\"])\n",
        "        acc_after.append(stats[\"acc_after\"])\n",
        "\n",
        "    #logger.log(\"=\" * 20, \"RESULTS\", \"=\" * 20, style=\"bold green\")\n",
        "    #logger.log(f\"loss_before_pers: {(sum(loss_before) / len(loss_before)):.4f}\")\n",
        "    #logger.log(f\"acc_before_pers: {(sum(acc_before) * 100.0 / len(acc_before)):.2f}%\")\n",
        "    #logger.log(f\"loss_after_pers: {(sum(loss_after) / len(loss_after)):.4f}\")\n",
        "    #logger.log(f\"acc_after_pers: {(sum(acc_after) * 100.0 / len(acc_after)):.2f}%\")\n",
        "\n",
        "    if args.log:\n",
        "        algo = \"HF\" if args.hf else \"FO\"\n",
        "        # logger.save_html(\n",
        "        #     f\"./log/{args.dataset}_{args.client_num_per_round}_{args.global_epochs}_{pers_epochs}_{algo}.html\"\n",
        "        # )"
      ],
      "metadata": {
        "id": "FM-3kT5QTaDu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cd98169ebeeb40bea353a3045ee62886",
            "4026b899edc540459973a2d1e769fbb9",
            "0b1ac056cd364a6a831b5f5952de9f0a",
            "5eaaf327bbbc40df9e1263a7496e6df5"
          ]
        },
        "outputId": "9483093e-2c8c-41e9-b04f-53adc482b0ac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}